{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T14:22:40.228770Z",
     "iopub.status.busy": "2024-09-29T14:22:40.228427Z",
     "iopub.status.idle": "2024-09-29T14:27:04.865968Z",
     "shell.execute_reply": "2024-09-29T14:27:04.865046Z",
     "shell.execute_reply.started": "2024-09-29T14:22:40.228742Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install trl wandb accelerate bitsandbytes \n",
    "!pip install torch==2.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T14:27:04.868044Z",
     "iopub.status.busy": "2024-09-29T14:27:04.867752Z",
     "iopub.status.idle": "2024-09-29T14:27:05.475279Z",
     "shell.execute_reply": "2024-09-29T14:27:05.474390Z",
     "shell.execute_reply.started": "2024-09-29T14:27:04.868017Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"your_token_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary method\n",
    "\n",
    "Particolarmente importante è la funzione di formattazione *formatting_func_alt* che permette di realizzare la formattazione del dataset in maniera opportuna secondo ciò che è ripostato nel datast _link_. \\\n",
    "Il dataset (estratto dal sito https://www.jefit.com/routines) è in formato JSON ed è composto da coppie <Question,Answer> dove la prima è una stringa e la seconda un oggetto routine composto da titolo e lista degli allenamenti. Gli allenamenti sono delle tabelle composte da cinque feature <Esercizio,Reps,Sets,Rest,Interval> dove l'esercizio consiste in un oggetto caratterizzato da un nome e un gruppo muscolare target. \\\n",
    "L'output della formatting_func_alt consiste in una stringa formata da tanti elementi tante quante sono le righe nelle tabelle e hanno la forma \\\n",
    "{'i',Excercise:'Nome Esercizio',Reps:'# ripetizioni',Sets:'# set', Interval:'durata esercizio se prevista', Rest:'Durata della pausa finito l'esercizio', Day:'giorno di riferimento'} \\\n",
    "Da notare che l'indice *i* è univoco per ogni riga e che il nome corrisponde al solo nome dell'esercizio (viene eliminata l'informazione sul gruppo muscolare di riferimento). \\\n",
    "Da notare che la domanda è invece ottenuta combinando la domanda originale del dataset con il nome della routine nella ground truth per differenziare maggiormente gli esempi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T14:27:05.477209Z",
     "iopub.status.busy": "2024-09-29T14:27:05.476575Z",
     "iopub.status.idle": "2024-09-29T14:27:07.317078Z",
     "shell.execute_reply": "2024-09-29T14:27:07.316263Z",
     "shell.execute_reply.started": "2024-09-29T14:27:05.477163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# metodo che effettua la formattazione delle coppie (Question,Answer) del dataset originale andando a creare una coppia di messaggi associati ad una domanda ottenuta come\n",
    "# concatenazione della Question originale e del nome del piano contenuto in Answer e ad una risposta che consiste in una versione serializzata della tabella originale.\n",
    "def formatting_func_alt(example):\n",
    "    answer = example['Answer']['Routine']\n",
    "    name = example['Answer']['Plan Name']\n",
    "    name = name.replace('- JEFIT','')\n",
    "    question = f'{example[\"Question\"]} {name}'\n",
    "    answ=[]\n",
    "    d=1\n",
    "    c=0\n",
    "    for routine in answer:\n",
    "        for excercise in routine:\n",
    "            tmp = excercise['Exercise']['Name']\n",
    "            excercise['Day']=d\n",
    "            excercise['Exercise']=tmp\n",
    "            tmp = {c:excercise}\n",
    "            c+=1\n",
    "            answ.append(tmp)\n",
    "        d+=1\n",
    "    ris = [{\"role\":\"user\",\"content\": question},{\"role\":\"assistant\",\"content\": answ}]\n",
    "    return ris\n",
    "\n",
    "# applica ad un esempio del dataset la formattazione e la trasformazione in prompt nella forma richiesta da mistral78 instruct e successivamente la tokenizza\n",
    "def generate_and_tokenize_prompt(prompt,max_length =  2048):\n",
    "    pairs = tokenizer.apply_chat_template(\n",
    "        formatting_func_alt(prompt),\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False)\n",
    "    \n",
    "    result = tokenizer(\n",
    "        pairs,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# metodo ausiliario per il conteggio del numero di parametri addestrabbili del modello\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset\n",
    "Osservazione si è lasciata la porzione finale del dataset come validation set per ogni esecuzione di addestramento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-29T14:27:07.320124Z",
     "iopub.status.busy": "2024-09-29T14:27:07.319720Z",
     "iopub.status.idle": "2024-09-29T14:27:11.777859Z",
     "shell.execute_reply": "2024-09-29T14:27:11.777112Z",
     "shell.execute_reply.started": "2024-09-29T14:27:07.320092Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='/kaggle/input/bb-routines/dataset.json', split='train[:5%]')\n",
    "eval_dataset = load_dataset('json', data_files='/kaggle/input/bb-routines/dataset.json', split='train[-1%:]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T14:27:11.779311Z",
     "iopub.status.busy": "2024-09-29T14:27:11.778961Z",
     "iopub.status.idle": "2024-09-29T14:27:11.783994Z",
     "shell.execute_reply": "2024-09-29T14:27:11.783193Z",
     "shell.execute_reply.started": "2024-09-29T14:27:11.779277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T14:27:11.785663Z",
     "iopub.status.busy": "2024-09-29T14:27:11.785270Z",
     "iopub.status.idle": "2024-09-29T14:28:39.646156Z",
     "shell.execute_reply": "2024-09-29T14:28:39.645308Z",
     "shell.execute_reply.started": "2024-09-29T14:27:11.785633Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quant_config, device_map=\"auto\",low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T14:28:39.648710Z",
     "iopub.status.busy": "2024-09-29T14:28:39.647634Z",
     "iopub.status.idle": "2024-09-29T14:28:41.051311Z",
     "shell.execute_reply": "2024-09-29T14:28:41.050309Z",
     "shell.execute_reply.started": "2024-09-29T14:28:39.648669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applicazione della trasformazione e della tokenizzazione degli esempi del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T14:28:41.052984Z",
     "iopub.status.busy": "2024-09-29T14:28:41.052511Z",
     "iopub.status.idle": "2024-09-29T14:28:43.749449Z",
     "shell.execute_reply": "2024-09-29T14:28:43.748551Z",
     "shell.execute_reply.started": "2024-09-29T14:28:41.052960Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T14:28:43.751432Z",
     "iopub.status.busy": "2024-09-29T14:28:43.750795Z",
     "iopub.status.idle": "2024-09-29T14:28:45.661592Z",
     "shell.execute_reply": "2024-09-29T14:28:45.660604Z",
     "shell.execute_reply.started": "2024-09-29T14:28:43.751396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T14:28:45.665832Z",
     "iopub.status.busy": "2024-09-29T14:28:45.665079Z",
     "iopub.status.idle": "2024-09-29T14:28:45.809246Z",
     "shell.execute_reply": "2024-09-29T14:28:45.806219Z",
     "shell.execute_reply.started": "2024-09-29T14:28:45.665798Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T14:28:45.811858Z",
     "iopub.status.busy": "2024-09-29T14:28:45.811437Z",
     "iopub.status.idle": "2024-09-29T14:28:46.737959Z",
     "shell.execute_reply": "2024-09-29T14:28:46.736938Z",
     "shell.execute_reply.started": "2024-09-29T14:28:45.811820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Attenzione: eseguendo il codice così come si sovrascrivono i pesi memorizzati nella repo huggingface finito il training del modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T14:28:46.741087Z",
     "iopub.status.busy": "2024-09-29T14:28:46.740820Z",
     "iopub.status.idle": "2024-09-29T14:28:49.580865Z",
     "shell.execute_reply": "2024-09-29T14:28:49.579944Z",
     "shell.execute_reply.started": "2024-09-29T14:28:46.741064Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"your_key_here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-29T14:28:49.582923Z",
     "iopub.status.busy": "2024-09-29T14:28:49.582226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name # path output locale dei checkpoint del modello\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=6,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=105,\n",
    "        learning_rate=1e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=15,\n",
    "        logging_dir=\"./logs\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=105,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=15,\n",
    "        do_eval=True,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=\"mistral_gym_training_1\",\n",
    "        max_grad_norm=1.5,\n",
    "        hub_strategy=\"checkpoint\",\n",
    "        hub_model_id=\"fabritmp/gym_mistral_finetune\", #nome repository huggingface\n",
    "        hub_token=\"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\",\n",
    "        push_to_hub= True\n",
    "    )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    peft_config=config,\n",
    "    max_seq_length= None,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing= False,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"fabritmp/gym_mistral_finetune\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(model_id)\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map='auto',\n",
    "    quantization_config=nf4_config,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt, model):\n",
    "  encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "  model_inputs = encoded_input.to('cuda')\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "  decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "  return decoded_output[0].replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(generate_response(\"Advanced level Maintaining 3 day a week Barbell\", model))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5568354,
     "sourceId": 9340251,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
